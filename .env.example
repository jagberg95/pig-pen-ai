# Pig Pen Environment Variables
# Copy this file to .env and fill in your values

# ─── LLM Provider ──────────────────────────────────────────────────
# Choose your provider: groq (free cloud), ollama (free local), or anthropic (paid)
# If not set, auto-detects based on which API key is present.
# LLM_PROVIDER=groq

# ─── Groq (FREE — recommended) ────────────────────────────────────
# Get a free API key at https://console.groq.com
# GROQ_API_KEY=gsk_xxxxxxxxxxxxxxxxxxxx

# ─── Ollama (FREE — runs locally, no key needed) ──────────────────
# Install from https://ollama.com, then run: ollama pull llama3.1
# OLLAMA_URL=http://localhost:11434

# ─── Anthropic (PAID) ─────────────────────────────────────────────
# Only needed if you want to use Claude
# ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxxxxxxxxx

# ─── Optional Overrides ───────────────────────────────────────────
# Override the model (defaults depend on provider)
# PIGPEN_MODEL=llama-3.3-70b-versatile

# Max tokens per LLM response (default: 4096)
# PIGPEN_MAX_TOKENS=4096
